{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc5aca13-dc7b-4f1b-bdef-40ae3eac2ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de71fc7e-29ac-4fce-b86e-89c8dead0518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter setting\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 30\n",
    "batch_size = 512\n",
    "\n",
    "# device detection (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "29d66d68-578a-4a89-9a18-7efc25a12f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    '''\n",
    "    Dataset class for loading and processing images and labels from NumPy arrays.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, images, labels, transforms):\n",
    "        '''\n",
    "        Initialization of the dataset.\n",
    "\n",
    "        Args:\n",
    "            Images (numpy.ndarray): Numpy array with images.\n",
    "            labels (numpy.ndarray): NumPy array with labels.\n",
    "            transforms (torchvision.transforms, optional): Transforms to apply to images.\n",
    "        '''\n",
    "        \n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Returns the length of the dataset (number of images).\n",
    "        '''\n",
    "        \n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        Returns a single element from the dataset (image and label) at the given index.\n",
    "\n",
    "        Args:\n",
    "            index (int): The index of an element in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the transformed image and label.\n",
    "        '''\n",
    "        \n",
    "        image = self.images[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        # Apply transformations (if defined)\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        if isinstance(image, np.ndarray):\n",
    "            image = torch.from_numpy(image).float()\n",
    "\n",
    "        if isinstance(label, np.ndarray):\n",
    "            label = torch.from_numpy(label).long()\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6ce3bf80-7ea5-4b0c-a2f3-6a936b2570f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images and labels from .npy format to Numpy arrays\n",
    "images = np.load('./images.npy')\n",
    "labels = np.load('./labels.npy')\n",
    "\n",
    "# define transforms\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(), # to torch tensor\n",
    "    transforms.Grayscale() # to one channel\n",
    "])\n",
    "\n",
    "# defining and customizing the dataset\n",
    "dataset = ImageDataset(images, labels, transforms=data_transforms)\n",
    "\n",
    "# dividing the data set into training and test samples\n",
    "train_data, test_data = torch.utils.data.random_split(dataset, [int(0.8 * len(dataset)), int(0.2 * len(dataset))])\n",
    "\n",
    "# defining and customizing the train and test dataloaders\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "285c4a6e-4534-4ca6-8a26-e23619c85e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# load CNN model ResNet18\n",
    "model = models.resnet18(pretrained=False)\n",
    "\n",
    "# setting number of classes\n",
    "num_classes = 3\n",
    "\n",
    "# setting the first convolution layer\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "# number of input functions for the last layer\n",
    "num_features = model.fc.in_features\n",
    "\n",
    "# new fully connected layer\n",
    "model.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7358dce4-7ec8-4ec2-968f-6f9150f51a44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c2c88759-900a-4eac-927c-bcd8fa3cf484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log setting\n",
    "current_time = datetime.datetime.now()\n",
    "log_file = f'./model_checkpoint/training_log{current_time.strftime(\"%Y-%m-%d_%H-%M-%S\")}.txt'\n",
    "model_save_path = f'./model_checkpoint/model_{current_time.strftime(\"%Y-%m-%d_%H-%M-%S\")}.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5aa02b14-64f4-42d5-b228-beef9b6c5fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/30\n",
      "test_loss: 2.3550 | test_accuracy: 0.4011\n",
      "epoch 2/30\n",
      "test_loss: 3.1644 | test_accuracy: 0.4011\n",
      "epoch 3/30\n",
      "test_loss: 4.1389 | test_accuracy: 0.4017\n",
      "epoch 4/30\n",
      "test_loss: 3.7233 | test_accuracy: 0.4094\n",
      "epoch 5/30\n",
      "test_loss: 2.6399 | test_accuracy: 0.5312\n",
      "epoch 6/30\n",
      "test_loss: 1.0113 | test_accuracy: 0.7190\n",
      "epoch 7/30\n",
      "test_loss: 0.6281 | test_accuracy: 0.8111\n",
      "epoch 8/30\n",
      "test_loss: 0.8857 | test_accuracy: 0.7522\n",
      "epoch 9/30\n",
      "test_loss: 0.8071 | test_accuracy: 0.7796\n",
      "epoch 10/30\n",
      "test_loss: 0.5970 | test_accuracy: 0.8081\n",
      "epoch 11/30\n",
      "test_loss: 0.6772 | test_accuracy: 0.8111\n",
      "epoch 12/30\n",
      "test_loss: 0.7692 | test_accuracy: 0.8027\n",
      "epoch 13/30\n",
      "test_loss: 0.6352 | test_accuracy: 0.8206\n",
      "epoch 14/30\n",
      "test_loss: 0.6791 | test_accuracy: 0.8146\n",
      "epoch 15/30\n",
      "test_loss: 0.6546 | test_accuracy: 0.8200\n",
      "epoch 16/30\n",
      "test_loss: 0.6591 | test_accuracy: 0.8182\n",
      "epoch 17/30\n",
      "test_loss: 0.6563 | test_accuracy: 0.8158\n",
      "epoch 18/30\n",
      "test_loss: 0.7008 | test_accuracy: 0.8033\n",
      "epoch 19/30\n",
      "test_loss: 0.6816 | test_accuracy: 0.8217\n",
      "epoch 20/30\n",
      "test_loss: 0.6882 | test_accuracy: 0.8170\n",
      "epoch 21/30\n",
      "test_loss: 0.6914 | test_accuracy: 0.8116\n",
      "epoch 22/30\n",
      "test_loss: 0.7118 | test_accuracy: 0.8128\n",
      "epoch 23/30\n",
      "test_loss: 0.6733 | test_accuracy: 0.8122\n",
      "epoch 24/30\n",
      "test_loss: 0.7248 | test_accuracy: 0.8164\n",
      "epoch 25/30\n",
      "test_loss: 0.7325 | test_accuracy: 0.8146\n",
      "epoch 26/30\n",
      "test_loss: 0.7132 | test_accuracy: 0.8146\n",
      "epoch 27/30\n",
      "test_loss: 0.7400 | test_accuracy: 0.8152\n",
      "epoch 28/30\n",
      "test_loss: 0.7210 | test_accuracy: 0.8164\n",
      "epoch 29/30\n",
      "test_loss: 0.7010 | test_accuracy: 0.8152\n",
      "epoch 30/30\n",
      "test_loss: 0.7582 | test_accuracy: 0.8116\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        labels = labels.long()\n",
    "        \n",
    "        # gradient cleaning\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss calculation\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % 100 == 99:\n",
    "            average_loss = running_loss / 100\n",
    "            print(f\"[Batch {i + 1}/{len(train_loader)}] Loss: {average_loss:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    accuracy = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            labels = labels.long()\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            # calculation loss and accuracy on test data\n",
    "            test_loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            accuracy += (predicted == labels).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    accuracy /= total\n",
    "\n",
    "    # Recording data on training results to a log file\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(f'test_loss: {test_loss:.4f} | test_accuracy: {accuracy:.4f}')\n",
    "    \n",
    "    print(f'test_loss: {test_loss:.4f} | test_accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "73dc13b6-d607-4cbd-88d5-61f8feac1b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model, model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "752435d0-5aa4-4fec-8d30-3b272e9fed23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model and optimizer\n",
    "torch.save({'model': model, 'optimizer': optimizer}, (model_save_path + '_and_optim'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3af1e624-6be3-4e4a-a4a3-2518468bd766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model state\n",
    "torch.save(model.state_dict(), (model_save_path + 'state_dict'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394d575e-7bda-40c5-adbd-61c8baeb2a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57083b6f-8b08-4e3e-bce8-ba52c4c79bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856bd412-22a2-4236-9efd-ef08664dcfdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
